{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CIFAR10_VGG_Resnet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37a233945bad4bc4bf2e4209670d87c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e670d836ef747fdb5b1485f6452e26d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da3ac908cf814f29b26ec77a373ab494",
              "IPY_MODEL_b3dc124f58394f33a63a487506ad74f7"
            ]
          }
        },
        "1e670d836ef747fdb5b1485f6452e26d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da3ac908cf814f29b26ec77a373ab494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f4c6d32e8a324e7592638e8cfb6c9280",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b4f69811d65a4b918f3fb4413d0ba705"
          }
        },
        "b3dc124f58394f33a63a487506ad74f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b42fef5aa784a6ba0c0639d48d512a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 44759011.99it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6b59d147ad348318fd8a779de16e286"
          }
        },
        "f4c6d32e8a324e7592638e8cfb6c9280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b4f69811d65a4b918f3fb4413d0ba705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b42fef5aa784a6ba0c0639d48d512a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6b59d147ad348318fd8a779de16e286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTVbLfh0NGhc"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.optim.lr_scheduler import StepLR\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHJfzTq9NGhx",
        "outputId": "864062f3-602d-440e-8299-dab813d6f84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "37a233945bad4bc4bf2e4209670d87c7",
            "1e670d836ef747fdb5b1485f6452e26d",
            "da3ac908cf814f29b26ec77a373ab494",
            "b3dc124f58394f33a63a487506ad74f7",
            "f4c6d32e8a324e7592638e8cfb6c9280",
            "b4f69811d65a4b918f3fb4413d0ba705",
            "2b42fef5aa784a6ba0c0639d48d512a3",
            "d6b59d147ad348318fd8a779de16e286"
          ]
        }
      },
      "source": [
        "# using pretrained means to normalize the dataset\n",
        "pretrained_means = [0.4914, 0.4822, 0.4465]\n",
        "pretrained_stds= [0.247, 0.243, 0.261]\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# define tranforms on the training set\n",
        "# include resizing and horizontal flip\n",
        "transform = transforms.Compose([transforms.Resize((32,32)),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean = pretrained_means,\n",
        "                                                     std = pretrained_stds)])\n",
        "# load the data\n",
        "train = datasets.CIFAR10(root='./data', train=True, download=True, transform = transform)\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True, transform = transform)\n",
        "\n",
        "# percentage of the training data to be used in training\n",
        "TRAIN_VAL_RATIO = 0.90\n",
        "\n",
        "n_train_examples = int(len(train) * TRAIN_VAL_RATIO)\n",
        "n_valid_examples = len(train) - n_train_examples\n",
        "\n",
        "train, val = torch.utils.data.random_split(train, [n_train_examples, n_valid_examples])\n",
        "\n",
        "# applying the transform on validation set\n",
        "val = copy.deepcopy(val)\n",
        "val.dataset.transform = transform\n",
        "\n",
        "# defining appropriate dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37a233945bad4bc4bf2e4209670d87c7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGYf6Hj-NGhy",
        "outputId": "5ec99f36-a64f-4645-bb68-2b10bf9bfbf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "class LRFinder:\n",
        "    def __init__(self, model, optimizer, criterion, device):\n",
        "        \n",
        "        self.optimizer = optimizer\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        \n",
        "        torch.save(model.state_dict(), 'init_params.pt')\n",
        "\n",
        "    def range_test(self, iterator, end_lr = 10, num_iter = 100, smooth_f = 0.05, diverge_th = 5):\n",
        "        \n",
        "        lrs = []\n",
        "        losses = []\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "        \n",
        "        iterator = IteratorWrapper(iterator)\n",
        "        \n",
        "        for iteration in range(num_iter):\n",
        "\n",
        "            loss = self._train_batch(iterator)\n",
        "\n",
        "            #update lr\n",
        "            lr_scheduler.step()\n",
        "            \n",
        "            lrs.append(lr_scheduler.get_lr()[0])\n",
        "\n",
        "            if iteration > 0:\n",
        "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
        "                \n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "\n",
        "            losses.append(loss)\n",
        "            \n",
        "            if loss > diverge_th * best_loss:\n",
        "                print(\"Stopping early, the loss has diverged\")\n",
        "                break\n",
        "                       \n",
        "        #reset model to initial parameters\n",
        "        model.load_state_dict(torch.load('init_params.pt'))\n",
        "                    \n",
        "        return lrs, losses\n",
        "\n",
        "    def _train_batch(self, iterator):\n",
        "        \n",
        "        self.model.train()\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        x, y = iterator.get_batch()\n",
        "        \n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        \n",
        "        #y_pred, _ = self.model(x) original\n",
        "        y_pred = self.model(x)\n",
        "\n",
        "                \n",
        "        loss = self.criterion(y_pred, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "class ExponentialLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "class IteratorWrapper:\n",
        "    def __init__(self, iterator):\n",
        "        self.iterator = iterator\n",
        "        self._iterator = iter(iterator)\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            inputs, labels = next(self._iterator)\n",
        "        except StopIteration:\n",
        "            self._iterator = iter(self.iterator)\n",
        "            inputs, labels, *_ = next(self._iterator)\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def get_batch(self):\n",
        "        return next(self)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-de063c3b07ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LRScheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_LRScheduler' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhY2jAiXNGhz"
      },
      "source": [
        "def plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n",
        "    \n",
        "    if skip_end == 0:\n",
        "        lrs = lrs[skip_start:]\n",
        "        losses = losses[skip_start:]\n",
        "    else:\n",
        "        lrs = lrs[skip_start:-skip_end]\n",
        "        losses = losses[skip_start:-skip_end]\n",
        "    \n",
        "    fig = plt.figure(figsize = (16,8))\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.plot(lrs, losses)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Learning rate')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True, 'both', 'x')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6Wf6R3NGh0"
      },
      "source": [
        "def get_preds(model, iterator):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            y_prob = F.softmax(y_pred, dim = -1)\n",
        "            top_pred = y_prob.argmax(1, keepdim = True)\n",
        "\n",
        "            images.append(x.cpu())\n",
        "            labels.append(y.cpu())\n",
        "            probs.append(y_prob.cpu())\n",
        "\n",
        "    images = torch.cat(images, dim = 0)\n",
        "    labels = torch.cat(labels, dim = 0)\n",
        "    probs = torch.cat(probs, dim = 0)\n",
        "\n",
        "    return images, labels, probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCZXnTXkNGh0"
      },
      "source": [
        "def plot_confusion_matrix(labels, pred_labels, classes):\n",
        "    \n",
        "    fig = plt.figure(figsize = (10, 10));\n",
        "    ax = fig.add_subplot(1, 1, 1);\n",
        "    cm = confusion_matrix(labels, pred_labels);\n",
        "    cm = ConfusionMatrixDisplay(cm, classes);\n",
        "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
        "    plt.xticks(rotation = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngBsGGFfNGh1"
      },
      "source": [
        "def labels_accuracy(y, y_pred):\n",
        "    crr = 0\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == y_pred[i]:\n",
        "            crr += 1\n",
        "    return crr/len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNdDgRBHNGh1"
      },
      "source": [
        "def normalize_image(image):\n",
        "    image_min = image.min()\n",
        "    image_max = image.max()\n",
        "    image.clamp_(min = image_min, max = image_max)\n",
        "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
        "    return image\n",
        "\n",
        "def plot_images(images, labels, classes, normalize = False):\n",
        "\n",
        "    n_images = len(images)\n",
        "\n",
        "    rows = int(np.sqrt(n_images))\n",
        "    cols = int(np.sqrt(n_images))\n",
        "\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "\n",
        "    for i in range(rows*cols):\n",
        "\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        \n",
        "        image = images[i]\n",
        "\n",
        "        if normalize:\n",
        "            image = normalize_image(image)\n",
        "\n",
        "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "        ax.set_title(classes[labels[i]])\n",
        "        ax.axis('off')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP83y2Z4NGh2"
      },
      "source": [
        "def show_incorrect_preds(incorrect, classes, n_images, normalize = False):\n",
        "\n",
        "    rows = 1 \n",
        "    cols = n_images  \n",
        "\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "    fig.tight_layout()\n",
        "    for i in range(cols):\n",
        "\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        \n",
        "        image, true_label, probs = incorrect[i]\n",
        "        image = image[0]\n",
        "        true_prob = probs[true_label]\n",
        "        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\n",
        "        true_class = classes[true_label][0]\n",
        "        incorrect_class = classes[incorrect_label][0]\n",
        "\n",
        "        if normalize:\n",
        "            image = normalize_image(image)\n",
        "\n",
        "        ax.imshow(image.cpu().numpy())\n",
        "        ax.set_title(f'true class: {true_class}\\n' \\\n",
        "                     f'pred class: {incorrect_class}')\n",
        "        ax.axis('off')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A63CFXWxNGh2"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roYgAiN4NGh2"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, features, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.features = features\n",
        "        \n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 , 128),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        #x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.classifier(h)\n",
        "        return x#, h"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rwnj81QNGh3"
      },
      "source": [
        "def get_vgg_layers(config, batch_norm):\n",
        "    \n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    \n",
        "    for c in config:\n",
        "        assert c == 'M' or isinstance(c, int)\n",
        "        if c == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size = 2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace = True)]\n",
        "            in_channels = c\n",
        "            \n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vtIJ1jwNGh3"
      },
      "source": [
        "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
        "\n",
        "vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixwfkWKwNGh3"
      },
      "source": [
        "vgg13_layers = get_vgg_layers(vgg13_config, batch_norm = True)\n",
        "vgg16_layers = get_vgg_layers(vgg16_config, batch_norm = True)\n",
        "vgg19_layers = get_vgg_layers(vgg19_config, batch_norm = True)\n",
        "\n",
        "vgg16_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgN9V4AMNGh4"
      },
      "source": [
        "OUTPUT_DIM = 10\n",
        "\n",
        "model = VGG(vgg13_layers, OUTPUT_DIM)\n",
        "model = VGG(vgg16_layers, OUTPUT_DIM)\n",
        "model = VGG(vgg19_layers, OUTPUT_DIM)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b32d_8tuNGh4"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UylbpHxPNGh4"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model.to(device), (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPH8iyaxNGh4"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT_pH7ZHNGh4"
      },
      "source": [
        "### Finding best LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_3LdNnkNGh5"
      },
      "source": [
        "START_LR = 1e-7\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = START_LR)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv3S-_hQNGh5"
      },
      "source": [
        "END_LR = 10\n",
        "NUM_ITER = 100\n",
        "\n",
        "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
        "lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgE8FFKzNGh6"
      },
      "source": [
        "plot_lr_finder(lrs, losses, skip_start = 10, skip_end = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebx6UcABNGh6"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZVpslENGh6"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001, weight_decay=0.0001)\n",
        "# scheduler for VGG\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10], last_epoch= -1)\n",
        "# scheduler for ResNet\n",
        "##scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[100, 150], last_epoch= -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxBQwyGINGh6"
      },
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    '''get model accuracy'''\n",
        "    p = y_pred.argmax(1, keepdim = True)\n",
        "    acc = p.eq(y.view_as(p)).sum().float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    '''\n",
        "    function to be called for training and collect \n",
        "    model loss and model accuracy\n",
        "    and perform a training step\n",
        "\n",
        "    iterator: DataLoader Object\n",
        "    optimizer: Optimizer type\n",
        "    criterion: loss type\n",
        "    '''\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set to training mode\n",
        "    model.train()\n",
        "    \n",
        "    for (x, y) in iterator:\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update loss and accuracy values\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    '''\n",
        "    function to be called for evaluating and collect \n",
        "    model val loss and model val accuracy\n",
        "\n",
        "    iterator: DataLoader Object\n",
        "    optimizer: Optimizer type\n",
        "    criterion: loss type\n",
        "    '''\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "EPOCHS = 20\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    #optimizer.step()\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    end_time = time.time()\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n",
        "    \n",
        "    #print(f'epoch: {epoch+1:02}, Time: {epoch_mins}m {epoch_secs}s')\n",
        "    #print(f'\\t train accuracy: {train_acc:.3f} | train loss: {train_loss:.3f} | ')\n",
        "    #print(f'\\t val accuracy: {val_acc:.3f} | val loss: {val_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMGEvkCnNGh7"
      },
      "source": [
        "images, labels, probs = get_preds(model, test_loader)\n",
        "pred_labels = torch.argmax(probs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ195jESNGh7"
      },
      "source": [
        "classes = test.classes\n",
        "\n",
        "plot_confusion_matrix(labels, pred_labels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mSlRO_NNGh7"
      },
      "source": [
        "plt.plot([i for i in range(1,EPOCHS + 1)], train_loss_list, label = 'training loss')\n",
        "plt.plot([i for i in range(1,EPOCHS + 1)], val_loss_list, label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.title('VGG1 , Cross Entropy Loss across Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzVMwPvfNGh8"
      },
      "source": [
        "plt.plot([i for i in range(1,EPOCHS + 1)], train_acc_list, label = 'training accuracy')\n",
        "plt.plot([i for i in range(1,EPOCHS + 1)], val_acc_list, label = 'validation accuracy')\n",
        "plt.legend()\n",
        "plt.title('VGG1 , Accuracy across Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XbngY-ZNGh8"
      },
      "source": [
        "labels_accuracy(labels.numpy(), pred_labels.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gjyNig8NGh8"
      },
      "source": [
        "correct_pred = torch.eq(labels, pred_labels)\n",
        "incorrect_examples = []\n",
        "\n",
        "for image, label, prob, correct in zip(images, labels, probs, correct_pred):\n",
        "    if not correct:\n",
        "        incorrect_examples.append((image, label, prob))\n",
        "\n",
        "incorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxIl-ZiCNGh8"
      },
      "source": [
        "show_incorrect_preds(incorrect_examples, classes, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EA-K8Q0NGh9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyXeMzFFNGh9"
      },
      "source": [
        "## Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXsXOCbKNGh9"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, output_dim):\n",
        "        super().__init__()\n",
        "                \n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "            \n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        \n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "        \n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n",
        "    \n",
        "        layers = []\n",
        "        \n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        \n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        \n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        \n",
        "        return x#, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rq-aAoLNGh-"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        \n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "                        \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjsXRXyqNGh-"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    \n",
        "    expansion = 4\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1, stride = 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "            \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "                \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "            \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33NV7-K9NGh_"
      },
      "source": [
        "class CIFARResNet(nn.Module):\n",
        "    def __init__(self, config, output_dim):\n",
        "        super().__init__()\n",
        "                \n",
        "        block, layers, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "            \n",
        "        assert len(layers) == len(channels) == 3\n",
        "        assert all([i == j*2 for i, j in zip(channels[1:], channels[:-1])])\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        self.layer1 = self.get_resnet_layer(block, layers[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, layers[1], channels[1], stride = 2)\n",
        "        self.layer3 = self.get_resnet_layer(block, layers[2], channels[2], stride = 2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "        \n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n",
        "    \n",
        "        layers = []\n",
        "        \n",
        "        if self.in_channels != channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        \n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        \n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(channels, channels))\n",
        "\n",
        "        self.in_channels = channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        \n",
        "        return x#, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1Rur5FNGh_"
      },
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self, f):\n",
        "        super().__init__()\n",
        "        self.f = f\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.f(x)\n",
        "        \n",
        "\n",
        "class CIFARBasicBlock(nn.Module):\n",
        "        \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3,stride = stride, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3,stride = 1, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "                \n",
        "        \n",
        "        if downsample:\n",
        "            identity_fn = lambda x : F.pad(x[:, :, ::2, ::2], [0, 0, 0, 0, in_channels // 2, in_channels // 2])\n",
        "            downsample = Identity(identity_fn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "                                \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-437eRMNGiA"
      },
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSm1QogWNGiA"
      },
      "source": [
        "resnet18_config = ResNetConfig (block = BasicBlock, n_blocks = [2,2,2,2], channels = [64, 128, 256, 512])\n",
        "\n",
        "resnet34_config = ResNetConfig (block = BasicBlock, n_blocks = [3,4,6,3], channels = [64, 128, 256, 512])\n",
        "\n",
        "\n",
        "resnet50_config = ResNetConfig (block = Bottleneck,n_blocks = [3, 4, 6, 3], channels = [64, 128, 256, 512])\n",
        "\n",
        "\n",
        "cifar_resnet20_config = ResNetConfig (block = CIFARBasicBlock, n_blocks = [3, 3, 3], channels = [16, 32, 64])\n",
        "\n",
        "cifar_resnet32_config = ResNetConfig (block = CIFARBasicBlock, n_blocks = [5, 5, 5], channels = [16, 32, 64])\n",
        "\n",
        "cifar_resnet44_config = ResNetConfig (block = CIFARBasicBlock, n_blocks = [7, 7, 7], channels = [16, 32, 64])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evxXMykRNGiA"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW9PTpw_NGiA"
      },
      "source": [
        "#model = CIFARResNet(cifar_resnet20_config,10)\n",
        "model = CIFARResNet(cifar_resnet44_config,10)\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(model.to(device), (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBGQBDa-NGiA"
      },
      "source": [
        "model = model.to(device)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDR2AysFNGiB"
      },
      "source": [
        "### Fiding best LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAy0CQpmNGiB"
      },
      "source": [
        "START_LR = 1e-7\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = START_LR)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGjOXDleNGiB"
      },
      "source": [
        "END_LR = 10\n",
        "NUM_ITER = 100\n",
        "\n",
        "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
        "lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5jwaA2BNGiB"
      },
      "source": [
        "plot_lr_finder(lrs, losses, skip_start = 10, skip_end = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5_bJdtCNGiB"
      },
      "source": [
        "###  Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFzCvHxbNGiB"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001, weight_decay=0.0001)\n",
        "# scheduler for VGG\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10], last_epoch= -1)\n",
        "# scheduler for ResNet\n",
        "##scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[100, 150], last_epoch= -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7TW-cshNGiC"
      },
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    '''get model accuracy'''\n",
        "    p = y_pred.argmax(1, keepdim = True)\n",
        "    acc = p.eq(y.view_as(p)).sum().float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    '''\n",
        "    function to be called for training and collect \n",
        "    model loss and model accuracy\n",
        "    and perform a training step\n",
        "\n",
        "    iterator: DataLoader Object\n",
        "    optimizer: Optimizer type\n",
        "    criterion: loss type\n",
        "    '''\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set to training mode\n",
        "    model.train()\n",
        "    \n",
        "    for (x, y) in iterator:\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update loss and accuracy values\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    '''\n",
        "    function to be called for evaluating and collect \n",
        "    model val loss and model val accuracy\n",
        "\n",
        "    iterator: DataLoader Object\n",
        "    optimizer: Optimizer type\n",
        "    criterion: loss type\n",
        "    '''\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "EPOCHS = 20\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    #optimizer.step()\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    end_time = time.time()\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n",
        "    \n",
        "    #print(f'epoch: {epoch+1:02}, Time: {epoch_mins}m {epoch_secs}s')\n",
        "    #print(f'\\t train accuracy: {train_acc:.3f} | train loss: {train_loss:.3f} | ')\n",
        "    #print(f'\\t val accuracy: {val_acc:.3f} | val loss: {val_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "551DHVHBNGiC"
      },
      "source": [
        "images, labels, probs = get_preds(model, test_loader)\n",
        "pred_labels = torch.argmax(probs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gE-3T5LNGiC"
      },
      "source": [
        "classes = test.classes\n",
        "\n",
        "plot_confusion_matrix(labels, pred_labels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuS3aqYhNGiC"
      },
      "source": [
        "plt.plot([i for i in range(1,EPOCHS + 1)], train_loss_list, label = 'training loss')\n",
        "plt.plot([i for i in range(1,EPOCHS + 1)], val_loss_list, label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.title('VGG1 , Cross Entropy Loss across Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQrl83ozNGiD"
      },
      "source": [
        "plt.plot([i for i in range(1,EPOCHS + 1)], train_acc_list, label = 'training accuracy')\n",
        "plt.plot([i for i in range(1,EPOCHS + 1)], val_acc_list, label = 'validation accuracy')\n",
        "plt.legend()\n",
        "plt.title('VGG1 , Accuracy across Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlbmARlRNGiD"
      },
      "source": [
        "labels_accuracy(labels.numpy(), pred_labels.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKie_zDyNGiD"
      },
      "source": [
        "correct_pred = torch.eq(labels, pred_labels)\n",
        "incorrect_examples = []\n",
        "\n",
        "for image, label, prob, correct in zip(images, labels, probs, correct_pred):\n",
        "    if not correct:\n",
        "        incorrect_examples.append((image, label, prob))\n",
        "\n",
        "incorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omdMieN5NGiD"
      },
      "source": [
        "show_incorrect_preds(incorrect_examples, classes, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXl7qDk9NGiD"
      },
      "source": [
        "## Comparing Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6Tp3GYNGiD"
      },
      "source": [
        "import seaborn as sns\n",
        "with sns.color_palette(\"husl\"):\n",
        "  \n",
        "    sns.lineplot([i for i in range(1,EPOCHS + 1)], train_acc_list, label = 'Training Accuracy')\n",
        "    sns.lineplot([i for i in range(1,EPOCHS + 1)], val_20, label = 'ResNet-20')\n",
        "    plt.plot([i for i in range(1,EPOCHS + 1)], val_44, label = 'ResNet-44')\n",
        "    plt.plot([i for i in range(1,EPOCHS + 1)], val_16, label = 'VGG-16')\n",
        "    plt.plot([i for i in range(1,EPOCHS + 1)], val_13, label = 'VGG-13')\n",
        "    plt.legend()\n",
        "    plt.title('Validation Accuracy on MNIST')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}