{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLEnd_starter_kit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-22UBGXn-1"
      },
      "source": [
        "# Environment set up\n",
        "\n",
        "In this section we will set up a Colab environment for the MLEnd mini-project. Before starting, follow these simple instructions: \n",
        "\n",
        "1.   Go to https://drive.google.com/\n",
        "2.   Create a folder named 'Data' in 'MyDrive'. On the left, click 'New' > 'Folder', enter the name 'Data', and click 'create'\n",
        "3.   Open the 'Data' folder and create a folder named 'MLEnd'.\n",
        "4.   Move the file 'trainingMLEnd.csv' to the newly created folder 'MyDrive/Data/MLEnd'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4NtuEQoxI4Y",
        "outputId": "977c5a93-a5ed-4782-d646-4b8b817e837c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, sys, re, pickle, glob\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "#from IPython.display import Audio\n",
        "import IPython.display as ipd\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g5c3_gKFjAgQMxyFlvx9dQ-rZFzCohVBVuuCRRCifz9V5cUjuwhF10\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i76_zeRbM7f"
      },
      "source": [
        "Run the following cell to check that the MLEnd folder contains the file 'trainingMLEnd,csv':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU-WbJenyt60",
        "outputId": "690abde2-184b-483c-ada8-db76ce3a8780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path = '/content/drive/MyDrive/Data/MLEnd'\n",
        "os.listdir(path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trainingMLEnd.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8QFyq3l4i4z"
      },
      "source": [
        "# Data download\n",
        "\n",
        "In this section we will download the data that you need to build your solutions. Note that even though we call it \"training\" dataset you can do whatever you want with it, for instance validation tasks. Note that we keep a separate dataset for testing purposes, which we won't share with anyone.\n",
        "\n",
        "First, we will define a function that will allow us to download a file into a chosen location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nKc1B94y0aq"
      },
      "source": [
        "def download_url(url, save_path):\n",
        "    with urllib.request.urlopen(url) as dl_file:\n",
        "        with open(save_path, 'wb') as out_file:\n",
        "            out_file.write(dl_file.read())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZNLz6L3WpdF"
      },
      "source": [
        "The next step is to download the file 'training.zip' into the folder 'MyDrive/Data/MLEnd'. Note that this might take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oktKxPugzdF3"
      },
      "source": [
        "url  = \"https://collect.qmul.ac.uk/down?t=6H8231DQL1NGDI9A/613DLM2R3OFV5EEH9INK2OG\"\n",
        "save_path = '/content/drive/MyDrive/Data/MLEnd/training.zip'\n",
        "download_url(url, save_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2qHL-drXD5t"
      },
      "source": [
        "Finally, let's unzip the training file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Hp5EPkzEY_"
      },
      "source": [
        "directory_to_extract_to = '/content/drive/MyDrive/Data/MLEnd/training/'\n",
        "with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r7X24B_c0tx"
      },
      "source": [
        "Once this step is completed, you should have all the audio files in the location 'MyDrive/Data/MLEnd/training/training'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ7D23Nq4pvZ"
      },
      "source": [
        "# Understanding our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyrwrtpqXhKD"
      },
      "source": [
        "Let's check how many audio files we have in our training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_3eb5Gm22BI"
      },
      "source": [
        "files = glob.glob('/content/drive/MyDrive/Data/MLEnd/training/*/*.wav')\n",
        "len(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnMMyS6vXkKR"
      },
      "source": [
        "This figure (20k) corresponds to the number of **items** or **samples** in our dataset. Let's listen to some random audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz60Gmni5EDu"
      },
      "source": [
        "# five random files\n",
        "for _ in range(5):\n",
        "    n = np.random.randint(20000)\n",
        "    display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7yhUEopd01C"
      },
      "source": [
        "Can you recognise the numeral and intonation? Can you recognise the speaker?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvLtzvuEeAgC"
      },
      "source": [
        "Let's now load the contents of 'trainingMLEnd.csv' into a pandas DataFrame and explore them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSe9A-XN5Kj6"
      },
      "source": [
        "labels = pd.read_csv('/content/drive/MyDrive/Data/MLEnd/trainingMLEnd.csv')\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_p3i6H2eYhU"
      },
      "source": [
        "This file consists of 20k rows and 4 columns. Each row corresponds to one of the items in our dataset, and each item is described by four attributes:\n",
        "\n",
        "\n",
        "1.   File ID (audio file)\n",
        "2.   Numeral\n",
        "3.   Participand ID\n",
        "4.   Intonation\n",
        "\n",
        "Could you explore this dataset further and identify how many items we have per numeral, per individual and per intonation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyIbZJym8C1K"
      },
      "source": [
        "# Feature extraction : Picth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haPg0E5jf83X"
      },
      "source": [
        "Audio files are complex data types. Specifically they are **discrete signals** or **time series**, consisting of values on a 1D grid. These values are known as *samples* themselves, which might be a bit confusing, as we have used this term to refer to the *items* in our dataset. The **sampling frequency** is the rate at which samples in an audio file are produced. For instance a sampling frequency of 5HZ indicates that 5 produce 5 samples per second, or 1 sample every 0.2 s.\n",
        "\n",
        "Let's plot one of our audio signals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEjYFsDf7DM8"
      },
      "source": [
        "n=0\n",
        "fs = None # Sampling frequency. If None, fs would be 22050\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "t = np.arange(len(x))/fs\n",
        "plt.plot(t,x)\n",
        "plt.xlabel('time (sec)')\n",
        "plt.ylabel('amplitude')\n",
        "plt.show()\n",
        "display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ZoeZDci4Wa"
      },
      "source": [
        "The file that we are listening to is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY5yH0_ti7j_"
      },
      "source": [
        "files[n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vjZ7gLi_5F"
      },
      "source": [
        "Can you recognise the numeral and intonation? Compare them with the values for the numeral and intonation that you can find in the `labels` DataFrame. By changing the value of `n` in the previous cell, you can listen to other examples. If you are doing this during one of our lab sessions, please make sure that your mic is muted!\n",
        "\n",
        "Exactly, how complex is an audio signal? Let's start by looking at the number of samples in one of our audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH2HfA94ki_x"
      },
      "source": [
        "n=0\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "print('This audio signal has', len(x), 'samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uCB_xj8k0VR"
      },
      "source": [
        "If we are using a raw audio signal as a predictor, we will be operating in a predictor space consisting of tens of thousands of dimensions. Compare this figure with the number of samples that we have. Do we have enough samples to train a model that takes one of these audio signals as an input?\n",
        "\n",
        "One approach is to extract a few features from our signals and use these features instead as predictors. In the following cell, we define a function that extracts four features from an audio signal, namely:\n",
        "\n",
        "\n",
        "1.   Power.\n",
        "2.   Pitch mean.\n",
        "3.   Pitch standard deviation.\n",
        "4.   Fraction of voiced region.\n",
        "\n",
        "In the next cell, we define a new function that gets the pitch of an audio signal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7swjqcap8BCi"
      },
      "source": [
        "def getPitch(x,fs,winLen=0.02):\n",
        "  #winLen = 0.02 \n",
        "    p = winLen*fs\n",
        "    frame_length = int(2**int(p-1).bit_length())\n",
        "    hop_length = frame_length//2\n",
        "    f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs, frame_length=frame_length,hop_length=hop_length)\n",
        "    return f0,voiced_flag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoLd6MDyn4pZ"
      },
      "source": [
        "Let's now consider the problem of identifying a numeral between 0 and 9. Then next cell defines a function that takes a number of files and creates a NumPy array containing the 4 audio features used as predictors (`X`) and their labels (`y`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7u8Yl359SQ_"
      },
      "source": [
        "def getXy(files,labels_file,scale_audio=False, onlySingleDigit=False):\n",
        "    X,y =[],[]\n",
        "    for file in tqdm(files):\n",
        "        fileID = file.split('/')[-1]\n",
        "        yi = list(labels_file[labels_file['File ID']==fileID]['digit_label'])[0]\n",
        "        if onlySingleDigit and yi>9:\n",
        "            continue\n",
        "        else:\n",
        "            fs = None # if None, fs would be 22050\n",
        "            x, fs = librosa.load(file,sr=fs)\n",
        "            if scale_audio: x = x/np.max(np.abs(x))\n",
        "            f0, voiced_flag = getPitch(x,fs,winLen=0.02)\n",
        "\n",
        "            power = np.sum(x**2)/len(x)\n",
        "            pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "            pitch_std  = np.nanstd(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "            voiced_fr = np.mean(voiced_flag)\n",
        "\n",
        "            xi = [power,pitch_mean,pitch_std,voiced_fr]\n",
        "            X.append(xi)\n",
        "            y.append(yi)\n",
        "    return np.array(X),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZQtxjdXnB2t"
      },
      "source": [
        "Let's apply `getXy` to the first 500 files. Note that the first 500 files contains numerals outside the [0, 9] range, which we wil be discaarding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8hjzCMb_THr"
      },
      "source": [
        "X,y = getXy(files[:500],labels_file=labels,scale_audio=True, onlySingleDigit=True)\n",
        "\n",
        "# If you want to use all 20000 files, run next line instead\n",
        "#X,y = getXy(files,labels_file=labels,scale_audio=True, onlySingleDigit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzzc9SBNpNAj"
      },
      "source": [
        "The next cell shows the shape of `X` and `y` and prints the labels vector `y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GxJh336FouZ"
      },
      "source": [
        "print('The shape of X is', X.shape) \n",
        "print('The shape of y is', y.shape)\n",
        "print('The labels vector is', y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO-4Vr00p7v6"
      },
      "source": [
        "Finally, to be on the cautious side, let's eliminate any potential item with a NaN (*not a number*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_vWpzO-HZn9"
      },
      "source": [
        "# If nan sample, remove them\n",
        "if np.sum(np.isnan(X)):\n",
        "    idx = np.isnan(X).sum(1)>0\n",
        "    X = X[~idx]\n",
        "    y = y[~idx]\n",
        "print(np.sum(np.isnan(X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsNkjyfFvAQ"
      },
      "source": [
        "# Modeling: Support Vector Machines\n",
        "\n",
        "Let's build a support vector machine (SVM) model for the predictive task of identifying digits in an audio signal, using the dataset that we have just created.\n",
        "\n",
        "We will use the SVM method provided by `scikit-learn` and will split the dataset defined by `X` and `y` into a training set and a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x2nXS1W_h9r"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKzWLxh0dON"
      },
      "source": [
        "Can you identify the number of items in the training and validation sets?\n",
        "\n",
        "Let's now fit an SVM model and print both the training accuracty and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwbJZTTiATWT"
      },
      "source": [
        "model  = svm.SVC(C=1)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "yt_p = model.predict(X_train)\n",
        "yv_p = model.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p==y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z794js6R2NZn"
      },
      "source": [
        "Compare the training and validation accuracies. What do you observe? What do you think the accuracy of a random classifier would be?\n",
        "\n",
        "Let's normalise the predictors, to see if the performance improves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixu4QncI2OOB"
      },
      "source": [
        "mean = X_train.mean(0)\n",
        "sd =  X_train.std(0)\n",
        "\n",
        "X_train = (X_train-mean)/sd\n",
        "X_val  = (X_val-mean)/sd\n",
        "\n",
        "model  = svm.SVC(C=1,gamma=2)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "yt_p = model.predict(X_train)\n",
        "yv_p = model.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p==y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2_8uRg036iC"
      },
      "source": [
        "Once again, compare the training and validation accuracies. Do you think this classifier is better than the previous one? What could you do to build a better classifier?"
      ]
    }
  ]
}