{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Moad Saad Khorchef<br>\n",
    "## ID: 200934655\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Solution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDjUBw3eIfrK",
    "outputId": "611c3087-6754-4b1f-ea93-18b1e5baea0e"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys, re, pickle, glob\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import soundfile\n",
    "\n",
    "\n",
    "#from IPython.display import Audio\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2FyOgx_rtiU"
   },
   "source": [
    "### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tfBxTkiCI3be"
   },
   "outputs": [],
   "source": [
    "def feature_chromagram(waveform, sample_rate):\n",
    "    # STFT computed here explicitly; mel spectrogram and MFCC functions do this under the hood\n",
    "    stft_spectrogram=np.abs(librosa.stft(waveform))\n",
    "    # Produce the chromagram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    chromagram=np.mean(librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate,hop_length=512,n_fft=2048).T,axis=0)\n",
    "    return chromagram\n",
    "\n",
    "def feature_melspectrogram(waveform, sample_rate):\n",
    "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
    "    melspectrogram=np.mean(librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128, fmax=sample_rate, hop_length=512,n_fft=2048).T,axis=0) ###\n",
    "    return melspectrogram\n",
    "\n",
    "def feature_mfcc(waveform, sample_rate):\n",
    "    # Compute the MFCCs for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    # 40 filterbanks = 40 coefficients\n",
    "    mfc_coefficients=np.mean(librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40).T, axis=0) \n",
    "    return mfc_coefficients\n",
    "\n",
    "\n",
    "\n",
    "def getPitch(x,fs,winLen=0.02):\n",
    "  #winLen = 0.02 \n",
    "    p = winLen*fs\n",
    "    frame_length = int(2**int(p-1).bit_length())\n",
    "    hop_length = frame_length//2\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs, frame_length=frame_length,hop_length=hop_length)\n",
    "    return f0,voiced_flag\n",
    "\n",
    "\n",
    "\n",
    "def get_features(file):\n",
    "    # load an individual soundfile\n",
    "     with soundfile.SoundFile(file) as audio:\n",
    "        waveform = audio.read(dtype=\"float32\")\n",
    "        sample_rate = audio.samplerate\n",
    "        # compute features of soundfile\n",
    "        chromagram = feature_chromagram(waveform, sample_rate)\n",
    "        melspectrogram = feature_melspectrogram(waveform, sample_rate)\n",
    "        mfc_coefficients = feature_mfcc(waveform, sample_rate)\n",
    "\n",
    "        # my added features\n",
    "        #######\n",
    "        y = waveform\n",
    "        sr = sample_rate\n",
    "        stft_=np.abs(librosa.stft(waveform))\n",
    "\n",
    "        cent = np.mean( librosa.feature.spectral_centroid(y=y, sr=sr,hop_length=512,n_fft=2048).T, axis=0)\n",
    "        contrast = np.mean( librosa.feature.spectral_contrast(S=stft_, sr=sr,hop_length=512,n_fft=2048).T, axis=0)\n",
    "        #tonnetz = np.mean( librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sample_rate ,hop_length=512).T,axis=0)#\n",
    "        rms = np.mean( librosa.feature.rms(y=y,frame_length=2048 ,hop_length=512).T, axis=0)\n",
    "        spec_bw = np.mean( librosa.feature.spectral_bandwidth(y=y, sr=sr ,n_fft=2048, hop_length=512).T, axis=0)\n",
    "        rolloff = np.mean( librosa.feature.spectral_rolloff(y=y, sr=sr ,n_fft=2048, hop_length=512).T, axis=0)\n",
    "        zcr = np.mean( librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512).T, axis=0) \n",
    "        #######\n",
    "\n",
    "        feature_matrix=np.array([])\n",
    "        # use np.hstack to stack our feature arrays horizontally to create a feature matrix\n",
    "        feature_matrix = np.hstack((chromagram, melspectrogram, mfc_coefficients , cent, contrast, rms, spec_bw, rolloff, zcr))\n",
    "        \n",
    "        return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s58-kfhrJALL"
   },
   "outputs": [],
   "source": [
    "#import os, glob\n",
    "\n",
    "#def load_data():\n",
    "#    X,y=[],[]\n",
    "#    count = 0\n",
    "#    for file in glob.glob('/content/drive/MyDrive/Data/MLEnd/training/*/*.wav'):\n",
    "#        file_name=os.path.basename(file)\n",
    "#        fileID = file.split('/')[-1]\n",
    "#        features = get_features(file)\n",
    "#        X.append(features)\n",
    "       # y.append(emotion)\n",
    "#        count += 1\n",
    "        # '\\r' + end='' results in printing over same line\n",
    "#        print('\\r' + f' Processed {count}/{20000} audio samples',end=' ')\n",
    "    # Return arrays to plug into sklearn's cross-validation algorithms\n",
    "#    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NSQvdCnFL0yT"
   },
   "outputs": [],
   "source": [
    "def getXy(files,labels_file,scale_audio=False, onlySingleDigit=False):\n",
    "    X,y =[],[]\n",
    "    for file in tqdm(files):\n",
    "        file = file.replace(\"\\\\\", \"/\")\n",
    "        fileID = file.split(\"/\")[-1]\n",
    "        #print(fileID)\n",
    "        yi = list(labels_file[labels_file['File ID']==fileID]['digit_label'])[0]\n",
    "        label = list(labels_file[labels_file['File ID']==fileID]['intonation'])[0]\n",
    "        if onlySingleDigit and yi>9:\n",
    "            continue\n",
    "        else:\n",
    "            fs = None # if None, fs would be 22050\n",
    "            x, fs = librosa.load(file,sr=fs)\n",
    "            if scale_audio: x = x/np.max(np.abs(x))\n",
    "            f0, voiced_flag = getPitch(x,fs,winLen=0.02)\n",
    "\n",
    "            power = np.sum(x**2)/len(x)\n",
    "            pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
    "            pitch_std  = np.nanstd(f0) if np.mean(np.isnan(f0))<1 else 0\n",
    "            voiced_fr = np.mean(voiced_flag)\n",
    "\n",
    "            #added \n",
    "            features = get_features(file)\n",
    "\n",
    "            #xi = [power,pitch_mean,pitch_std,voiced_fr]\n",
    "\n",
    "            xi = np.hstack((power,pitch_mean,pitch_std,voiced_fr,features))\n",
    "\n",
    "            X.append(xi)\n",
    "            y.append(label)\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>digit_label</th>\n",
       "      <th>participant</th>\n",
       "      <th>intonation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>S73</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000001.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>S88</td>\n",
       "      <td>excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000002.wav</td>\n",
       "      <td>70</td>\n",
       "      <td>S5</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>S85</td>\n",
       "      <td>bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000004.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>S30</td>\n",
       "      <td>excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0019995.wav</td>\n",
       "      <td>90</td>\n",
       "      <td>S163</td>\n",
       "      <td>excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0019996.wav</td>\n",
       "      <td>10</td>\n",
       "      <td>S99</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0019997.wav</td>\n",
       "      <td>90</td>\n",
       "      <td>S46</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0019998.wav</td>\n",
       "      <td>19</td>\n",
       "      <td>S13</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0019999.wav</td>\n",
       "      <td>20</td>\n",
       "      <td>S101</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           File ID  digit_label participant intonation\n",
       "0      0000000.wav            4         S73   question\n",
       "1      0000001.wav            2         S88    excited\n",
       "2      0000002.wav           70          S5    neutral\n",
       "3      0000003.wav            2         S85      bored\n",
       "4      0000004.wav            4         S30    excited\n",
       "...            ...          ...         ...        ...\n",
       "19995  0019995.wav           90        S163    excited\n",
       "19996  0019996.wav           10         S99   question\n",
       "19997  0019997.wav           90         S46   question\n",
       "19998  0019998.wav           19         S13    neutral\n",
       "19999  0019999.wav           20        S101    neutral\n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('./Data/MLEnd/trainingMLEnd.csv')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSRor5BeKNj-",
    "outputId": "49085e8b-3127-46b6-cc3d-b7384876e54c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6000 [00:00<?, ?it/s]C:\\Users\\Moad\\anaconda3\\lib\\site-packages\\librosa\\filters.py:238: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6000/6000 [14:54<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"./Data/MLEnd/training/Training/*.wav\" )\n",
    "\n",
    "\n",
    "X,y = getXy(files[:6000],labels_file=labels,scale_audio=True, onlySingleDigit=True)\n",
    "\n",
    "#a,b = getXy(files[:1000],labels_file=labels,scale_audio=True, onlySingleDigit=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('./Data/MLEnd/training/Training/*.wav')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5l2dL0wJaSpV"
   },
   "outputs": [],
   "source": [
    "#print('The shape of X is', a.shape) \n",
    "#print('The shape of y is', b.shape)\n",
    "#print('The labels vector is', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6x4I-Y-QAb9",
    "outputId": "6f4974a6-a3b1-4add-dcc5-26e8cbf248af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is (1949, 196)\n",
      "The shape of y is (1949,)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of X is', X.shape) \n",
    "print('The shape of y is', y.shape)\n",
    "#print('The labels vector is', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kki0oC8OrjGu"
   },
   "source": [
    "### Best Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0g0wTYEkf1Uy",
    "outputId": "1f5af75e-5ddc-42cd-f0c5-f44a4402a087"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1949, 190)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif#\n",
    "\n",
    "#X.shape\n",
    "X_new = SelectKBest(mutual_info_classif, k=190).fit_transform(X, y)\n",
    "#X_new = SelectKBest(mutual_info_classif, k=150).fit_transform(a, b)\n",
    "\n",
    "X_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wxHZ2cWnR0kU",
    "outputId": "07fc174f-180a-45fc-a500-8d5d3c4d9210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audio samples represented: 1949\n",
      "Numerical features extracted per sample: 190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010129</td>\n",
       "      <td>208.882834</td>\n",
       "      <td>118.829623</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>0.753131</td>\n",
       "      <td>0.766886</td>\n",
       "      <td>0.749057</td>\n",
       "      <td>0.671027</td>\n",
       "      <td>0.634170</td>\n",
       "      <td>0.670543</td>\n",
       "      <td>...</td>\n",
       "      <td>12.066502</td>\n",
       "      <td>13.726420</td>\n",
       "      <td>15.600197</td>\n",
       "      <td>15.085901</td>\n",
       "      <td>15.386061</td>\n",
       "      <td>19.041806</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>2197.995732</td>\n",
       "      <td>4693.939209</td>\n",
       "      <td>0.159003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035339</td>\n",
       "      <td>157.289085</td>\n",
       "      <td>31.784415</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.583537</td>\n",
       "      <td>0.643321</td>\n",
       "      <td>0.677088</td>\n",
       "      <td>0.669812</td>\n",
       "      <td>0.709702</td>\n",
       "      <td>0.740893</td>\n",
       "      <td>...</td>\n",
       "      <td>13.944263</td>\n",
       "      <td>15.772536</td>\n",
       "      <td>16.208509</td>\n",
       "      <td>17.446751</td>\n",
       "      <td>17.795718</td>\n",
       "      <td>27.592407</td>\n",
       "      <td>0.031027</td>\n",
       "      <td>2434.352641</td>\n",
       "      <td>5335.701069</td>\n",
       "      <td>0.155646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013676</td>\n",
       "      <td>134.964540</td>\n",
       "      <td>4.520547</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.677341</td>\n",
       "      <td>0.748570</td>\n",
       "      <td>0.695867</td>\n",
       "      <td>0.597716</td>\n",
       "      <td>0.595714</td>\n",
       "      <td>0.726472</td>\n",
       "      <td>...</td>\n",
       "      <td>12.323731</td>\n",
       "      <td>16.377326</td>\n",
       "      <td>15.793714</td>\n",
       "      <td>20.858538</td>\n",
       "      <td>21.230001</td>\n",
       "      <td>19.016606</td>\n",
       "      <td>0.031680</td>\n",
       "      <td>1446.833328</td>\n",
       "      <td>1627.869907</td>\n",
       "      <td>0.027166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.052517</td>\n",
       "      <td>181.944641</td>\n",
       "      <td>40.719889</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.537822</td>\n",
       "      <td>0.540425</td>\n",
       "      <td>0.537556</td>\n",
       "      <td>0.533592</td>\n",
       "      <td>0.509157</td>\n",
       "      <td>...</td>\n",
       "      <td>15.977014</td>\n",
       "      <td>20.075173</td>\n",
       "      <td>18.970150</td>\n",
       "      <td>17.982028</td>\n",
       "      <td>19.588298</td>\n",
       "      <td>25.813768</td>\n",
       "      <td>0.078083</td>\n",
       "      <td>1994.697128</td>\n",
       "      <td>3658.686967</td>\n",
       "      <td>0.105957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027461</td>\n",
       "      <td>109.742686</td>\n",
       "      <td>18.208426</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.704168</td>\n",
       "      <td>0.769274</td>\n",
       "      <td>0.750523</td>\n",
       "      <td>0.715473</td>\n",
       "      <td>0.715584</td>\n",
       "      <td>0.750014</td>\n",
       "      <td>...</td>\n",
       "      <td>11.006136</td>\n",
       "      <td>14.597893</td>\n",
       "      <td>15.629837</td>\n",
       "      <td>15.968503</td>\n",
       "      <td>17.372982</td>\n",
       "      <td>28.002703</td>\n",
       "      <td>0.073354</td>\n",
       "      <td>1915.549197</td>\n",
       "      <td>4090.016602</td>\n",
       "      <td>0.118665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>0.012427</td>\n",
       "      <td>218.320824</td>\n",
       "      <td>74.117411</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>0.671424</td>\n",
       "      <td>0.675539</td>\n",
       "      <td>0.718936</td>\n",
       "      <td>0.770489</td>\n",
       "      <td>0.783567</td>\n",
       "      <td>0.700486</td>\n",
       "      <td>...</td>\n",
       "      <td>13.106332</td>\n",
       "      <td>14.723138</td>\n",
       "      <td>14.927029</td>\n",
       "      <td>16.610873</td>\n",
       "      <td>16.490449</td>\n",
       "      <td>22.097368</td>\n",
       "      <td>0.038017</td>\n",
       "      <td>2287.243290</td>\n",
       "      <td>5410.558000</td>\n",
       "      <td>0.220672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>0.019418</td>\n",
       "      <td>244.020822</td>\n",
       "      <td>69.803627</td>\n",
       "      <td>0.337349</td>\n",
       "      <td>0.687252</td>\n",
       "      <td>0.701269</td>\n",
       "      <td>0.734410</td>\n",
       "      <td>0.739803</td>\n",
       "      <td>0.626294</td>\n",
       "      <td>0.625772</td>\n",
       "      <td>...</td>\n",
       "      <td>15.619662</td>\n",
       "      <td>16.779178</td>\n",
       "      <td>16.662217</td>\n",
       "      <td>16.648297</td>\n",
       "      <td>15.834053</td>\n",
       "      <td>25.801179</td>\n",
       "      <td>0.064158</td>\n",
       "      <td>2398.899065</td>\n",
       "      <td>4433.504918</td>\n",
       "      <td>0.065118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>0.055544</td>\n",
       "      <td>205.535959</td>\n",
       "      <td>44.520564</td>\n",
       "      <td>0.508065</td>\n",
       "      <td>0.500811</td>\n",
       "      <td>0.442746</td>\n",
       "      <td>0.422549</td>\n",
       "      <td>0.392942</td>\n",
       "      <td>0.397227</td>\n",
       "      <td>0.447477</td>\n",
       "      <td>...</td>\n",
       "      <td>18.152734</td>\n",
       "      <td>22.305314</td>\n",
       "      <td>22.854587</td>\n",
       "      <td>25.405481</td>\n",
       "      <td>27.847928</td>\n",
       "      <td>33.575963</td>\n",
       "      <td>0.165468</td>\n",
       "      <td>1813.729011</td>\n",
       "      <td>3224.423513</td>\n",
       "      <td>0.081496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>0.022832</td>\n",
       "      <td>270.207906</td>\n",
       "      <td>77.876257</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.584216</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.611628</td>\n",
       "      <td>0.667121</td>\n",
       "      <td>0.762832</td>\n",
       "      <td>0.667282</td>\n",
       "      <td>...</td>\n",
       "      <td>16.730680</td>\n",
       "      <td>18.322132</td>\n",
       "      <td>17.769921</td>\n",
       "      <td>18.166319</td>\n",
       "      <td>21.190978</td>\n",
       "      <td>22.895909</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>2193.073818</td>\n",
       "      <td>5535.166530</td>\n",
       "      <td>0.206826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>0.022271</td>\n",
       "      <td>209.188324</td>\n",
       "      <td>17.903261</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.698230</td>\n",
       "      <td>0.691233</td>\n",
       "      <td>0.631692</td>\n",
       "      <td>0.642415</td>\n",
       "      <td>0.635447</td>\n",
       "      <td>0.615306</td>\n",
       "      <td>...</td>\n",
       "      <td>13.454528</td>\n",
       "      <td>18.725646</td>\n",
       "      <td>14.281809</td>\n",
       "      <td>16.210192</td>\n",
       "      <td>17.468596</td>\n",
       "      <td>23.073855</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>2548.469437</td>\n",
       "      <td>5033.620287</td>\n",
       "      <td>0.096754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1949 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1           2         3         4         5    \\\n",
       "0     0.010129  208.882834  118.829623  0.215278  0.753131  0.766886   \n",
       "1     0.035339  157.289085   31.784415  0.293333  0.583537  0.643321   \n",
       "2     0.013676  134.964540    4.520547  0.084507  0.677341  0.748570   \n",
       "3     0.052517  181.944641   40.719889  0.494253  0.569682  0.537822   \n",
       "4     0.027461  109.742686   18.208426  0.120000  0.704168  0.769274   \n",
       "...        ...         ...         ...       ...       ...       ...   \n",
       "1944  0.012427  218.320824   74.117411  0.215190  0.671424  0.675539   \n",
       "1945  0.019418  244.020822   69.803627  0.337349  0.687252  0.701269   \n",
       "1946  0.055544  205.535959   44.520564  0.508065  0.500811  0.442746   \n",
       "1947  0.022832  270.207906   77.876257  0.302632  0.584216  0.621987   \n",
       "1948  0.022271  209.188324   17.903261  0.186813  0.698230  0.691233   \n",
       "\n",
       "           6         7         8         9    ...        180        181  \\\n",
       "0     0.749057  0.671027  0.634170  0.670543  ...  12.066502  13.726420   \n",
       "1     0.677088  0.669812  0.709702  0.740893  ...  13.944263  15.772536   \n",
       "2     0.695867  0.597716  0.595714  0.726472  ...  12.323731  16.377326   \n",
       "3     0.540425  0.537556  0.533592  0.509157  ...  15.977014  20.075173   \n",
       "4     0.750523  0.715473  0.715584  0.750014  ...  11.006136  14.597893   \n",
       "...        ...       ...       ...       ...  ...        ...        ...   \n",
       "1944  0.718936  0.770489  0.783567  0.700486  ...  13.106332  14.723138   \n",
       "1945  0.734410  0.739803  0.626294  0.625772  ...  15.619662  16.779178   \n",
       "1946  0.422549  0.392942  0.397227  0.447477  ...  18.152734  22.305314   \n",
       "1947  0.611628  0.667121  0.762832  0.667282  ...  16.730680  18.322132   \n",
       "1948  0.631692  0.642415  0.635447  0.615306  ...  13.454528  18.725646   \n",
       "\n",
       "            182        183        184        185       186          187  \\\n",
       "0     15.600197  15.085901  15.386061  19.041806  0.009657  2197.995732   \n",
       "1     16.208509  17.446751  17.795718  27.592407  0.031027  2434.352641   \n",
       "2     15.793714  20.858538  21.230001  19.016606  0.031680  1446.833328   \n",
       "3     18.970150  17.982028  19.588298  25.813768  0.078083  1994.697128   \n",
       "4     15.629837  15.968503  17.372982  28.002703  0.073354  1915.549197   \n",
       "...         ...        ...        ...        ...       ...          ...   \n",
       "1944  14.927029  16.610873  16.490449  22.097368  0.038017  2287.243290   \n",
       "1945  16.662217  16.648297  15.834053  25.801179  0.064158  2398.899065   \n",
       "1946  22.854587  25.405481  27.847928  33.575963  0.165468  1813.729011   \n",
       "1947  17.769921  18.166319  21.190978  22.895909  0.025193  2193.073818   \n",
       "1948  14.281809  16.210192  17.468596  23.073855  0.014502  2548.469437   \n",
       "\n",
       "              188       189  \n",
       "0     4693.939209  0.159003  \n",
       "1     5335.701069  0.155646  \n",
       "2     1627.869907  0.027166  \n",
       "3     3658.686967  0.105957  \n",
       "4     4090.016602  0.118665  \n",
       "...           ...       ...  \n",
       "1944  5410.558000  0.220672  \n",
       "1945  4433.504918  0.065118  \n",
       "1946  3224.423513  0.081496  \n",
       "1947  5535.166530  0.206826  \n",
       "1948  5033.620287  0.096754  \n",
       "\n",
       "[1949 rows x 190 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\nAudio samples represented: {X_new.shape[0]}')\n",
    "print(f'Numerical features extracted per sample: {X_new.shape[1]}')\n",
    "features_df = pd.DataFrame(X_new) # make it pretty for display\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptP8E8J5q2Ej"
   },
   "source": [
    "### Class Balance Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a34-98ZEnYlK",
    "outputId": "5763ef92-9261-4cae-e379-de885d603803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bored', 'excited', 'neutral', 'question'], dtype='<U8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intonation_list = np.unique(y)\n",
    "intonation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "XGpvh4EjnEmT",
    "outputId": "c0518534-3c93-467e-e0a8-19d8f4a0e3f1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2520x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot emotions\n",
    "plt.figure(figsize=(35,4))\n",
    "plt.subplot(1,3,1)\n",
    "#np.unique returns ordered list of unique elements and count of each element\n",
    "intonation_list, count = np.unique(y, return_counts=True)\n",
    "plt.bar(x=range(4), height=count)\n",
    "plt.xticks(ticks=range(4), labels = [intonation for intonation in intonation_list],fontsize=10)\n",
    "plt.xlabel('intonation')\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTIPPs_antQR",
    "outputId": "9280e681-847c-4f65-a124-0cffd081cc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# If nan sample, remove them\n",
    "if np.sum(np.isnan(X)):\n",
    "    idx = np.isnan(X).sum(1)>0\n",
    "    X = X[~idx]\n",
    "    y = y[~idx]\n",
    "print(np.sum(np.isnan(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYyyMvBJq9T7"
   },
   "source": [
    "### Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "epVppsnHo7NR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# keep our unscaled features just in case we need to process them alternatively\n",
    "#features_scaled = X\n",
    "#features_scaled = a\n",
    "features_scaled = X_new\n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "# keep our unscaled features just in case we need to process them alternatively\n",
    "#features_minmax = features\n",
    "#features_minmax = scaler.fit_transform(features_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "gGD6HgWLEgeD",
    "outputId": "89a866ca-d465-468c-c5fb-79e8d048f2a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.842976</td>\n",
       "      <td>0.400776</td>\n",
       "      <td>3.385139</td>\n",
       "      <td>-0.579043</td>\n",
       "      <td>0.799140</td>\n",
       "      <td>1.027576</td>\n",
       "      <td>0.857193</td>\n",
       "      <td>0.111030</td>\n",
       "      <td>-0.139985</td>\n",
       "      <td>0.260398</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.232015</td>\n",
       "      <td>-1.458734</td>\n",
       "      <td>-0.386383</td>\n",
       "      <td>-1.335666</td>\n",
       "      <td>-1.373209</td>\n",
       "      <td>-1.175288</td>\n",
       "      <td>-0.711562</td>\n",
       "      <td>-0.066101</td>\n",
       "      <td>-0.012779</td>\n",
       "      <td>0.290658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876839</td>\n",
       "      <td>-0.247284</td>\n",
       "      <td>0.142631</td>\n",
       "      <td>-0.091997</td>\n",
       "      <td>-0.860901</td>\n",
       "      <td>-0.174619</td>\n",
       "      <td>0.164147</td>\n",
       "      <td>0.099436</td>\n",
       "      <td>0.549521</td>\n",
       "      <td>0.899578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398668</td>\n",
       "      <td>-0.524623</td>\n",
       "      <td>-0.095913</td>\n",
       "      <td>-0.112957</td>\n",
       "      <td>-0.236553</td>\n",
       "      <td>0.978520</td>\n",
       "      <td>0.038124</td>\n",
       "      <td>0.557227</td>\n",
       "      <td>0.458210</td>\n",
       "      <td>0.248346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600979</td>\n",
       "      <td>-0.527699</td>\n",
       "      <td>-0.872971</td>\n",
       "      <td>-1.395019</td>\n",
       "      <td>0.057287</td>\n",
       "      <td>0.849369</td>\n",
       "      <td>0.344986</td>\n",
       "      <td>-0.588274</td>\n",
       "      <td>-0.491041</td>\n",
       "      <td>0.768550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117857</td>\n",
       "      <td>-0.248518</td>\n",
       "      <td>-0.293978</td>\n",
       "      <td>1.654044</td>\n",
       "      <td>1.383427</td>\n",
       "      <td>-1.181635</td>\n",
       "      <td>0.061036</td>\n",
       "      <td>-2.047093</td>\n",
       "      <td>-2.262965</td>\n",
       "      <td>-1.370896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.048664</td>\n",
       "      <td>0.062410</td>\n",
       "      <td>0.475485</td>\n",
       "      <td>1.161689</td>\n",
       "      <td>-0.996521</td>\n",
       "      <td>-1.201049</td>\n",
       "      <td>-1.151872</td>\n",
       "      <td>-1.162142</td>\n",
       "      <td>-1.058135</td>\n",
       "      <td>-1.205914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503463</td>\n",
       "      <td>1.439656</td>\n",
       "      <td>1.222777</td>\n",
       "      <td>0.164269</td>\n",
       "      <td>0.609022</td>\n",
       "      <td>0.530499</td>\n",
       "      <td>1.688916</td>\n",
       "      <td>-0.602248</td>\n",
       "      <td>-0.772550</td>\n",
       "      <td>-0.377887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.339394</td>\n",
       "      <td>-0.844506</td>\n",
       "      <td>-0.363086</td>\n",
       "      <td>-1.173552</td>\n",
       "      <td>0.319872</td>\n",
       "      <td>1.050802</td>\n",
       "      <td>0.871304</td>\n",
       "      <td>0.534995</td>\n",
       "      <td>0.603220</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.702604</td>\n",
       "      <td>-1.060881</td>\n",
       "      <td>-0.372230</td>\n",
       "      <td>-0.878557</td>\n",
       "      <td>-0.435961</td>\n",
       "      <td>1.081869</td>\n",
       "      <td>1.522988</td>\n",
       "      <td>-0.810979</td>\n",
       "      <td>-0.455997</td>\n",
       "      <td>-0.217723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>-0.686187</td>\n",
       "      <td>0.519325</td>\n",
       "      <td>1.719571</td>\n",
       "      <td>-0.579592</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>0.138839</td>\n",
       "      <td>0.567138</td>\n",
       "      <td>1.059792</td>\n",
       "      <td>1.223815</td>\n",
       "      <td>0.532453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.770540</td>\n",
       "      <td>-1.003703</td>\n",
       "      <td>-0.707822</td>\n",
       "      <td>-0.545867</td>\n",
       "      <td>-0.852259</td>\n",
       "      <td>-0.405623</td>\n",
       "      <td>0.283348</td>\n",
       "      <td>0.169265</td>\n",
       "      <td>0.513147</td>\n",
       "      <td>1.067873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>-0.209269</td>\n",
       "      <td>0.842139</td>\n",
       "      <td>1.558879</td>\n",
       "      <td>0.182652</td>\n",
       "      <td>0.154298</td>\n",
       "      <td>0.389172</td>\n",
       "      <td>0.716143</td>\n",
       "      <td>0.767077</td>\n",
       "      <td>-0.211884</td>\n",
       "      <td>-0.146379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344871</td>\n",
       "      <td>-0.065061</td>\n",
       "      <td>0.120734</td>\n",
       "      <td>-0.526485</td>\n",
       "      <td>-1.161887</td>\n",
       "      <td>0.527328</td>\n",
       "      <td>1.200383</td>\n",
       "      <td>0.463728</td>\n",
       "      <td>-0.203912</td>\n",
       "      <td>-0.892584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>2.255192</td>\n",
       "      <td>0.358737</td>\n",
       "      <td>0.617064</td>\n",
       "      <td>1.247870</td>\n",
       "      <td>-1.670651</td>\n",
       "      <td>-2.126057</td>\n",
       "      <td>-2.286985</td>\n",
       "      <td>-2.541593</td>\n",
       "      <td>-2.302972</td>\n",
       "      <td>-1.766326</td>\n",
       "      <td>...</td>\n",
       "      <td>1.469044</td>\n",
       "      <td>2.457780</td>\n",
       "      <td>3.077604</td>\n",
       "      <td>4.008954</td>\n",
       "      <td>4.505159</td>\n",
       "      <td>2.485716</td>\n",
       "      <td>4.754478</td>\n",
       "      <td>-1.079503</td>\n",
       "      <td>-1.091256</td>\n",
       "      <td>-0.686173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>0.023606</td>\n",
       "      <td>1.171070</td>\n",
       "      <td>1.859592</td>\n",
       "      <td>-0.033978</td>\n",
       "      <td>-0.854261</td>\n",
       "      <td>-0.382188</td>\n",
       "      <td>-0.466211</td>\n",
       "      <td>0.073773</td>\n",
       "      <td>1.034530</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837939</td>\n",
       "      <td>0.639342</td>\n",
       "      <td>0.649665</td>\n",
       "      <td>0.259715</td>\n",
       "      <td>1.365020</td>\n",
       "      <td>-0.204479</td>\n",
       "      <td>-0.166559</td>\n",
       "      <td>-0.079082</td>\n",
       "      <td>0.604597</td>\n",
       "      <td>0.893364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>-0.014666</td>\n",
       "      <td>0.404614</td>\n",
       "      <td>-0.374453</td>\n",
       "      <td>-0.756655</td>\n",
       "      <td>0.261750</td>\n",
       "      <td>0.291526</td>\n",
       "      <td>-0.272998</td>\n",
       "      <td>-0.161894</td>\n",
       "      <td>-0.128327</td>\n",
       "      <td>-0.241474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616012</td>\n",
       "      <td>0.823558</td>\n",
       "      <td>-1.015916</td>\n",
       "      <td>-0.753383</td>\n",
       "      <td>-0.390859</td>\n",
       "      <td>-0.159656</td>\n",
       "      <td>-0.541590</td>\n",
       "      <td>0.858180</td>\n",
       "      <td>0.236513</td>\n",
       "      <td>-0.493873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1949 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.842976  0.400776  3.385139 -0.579043  0.799140  1.027576  0.857193   \n",
       "1     0.876839 -0.247284  0.142631 -0.091997 -0.860901 -0.174619  0.164147   \n",
       "2    -0.600979 -0.527699 -0.872971 -1.395019  0.057287  0.849369  0.344986   \n",
       "3     2.048664  0.062410  0.475485  1.161689 -0.996521 -1.201049 -1.151872   \n",
       "4     0.339394 -0.844506 -0.363086 -1.173552  0.319872  1.050802  0.871304   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1944 -0.686187  0.519325  1.719571 -0.579592 -0.000634  0.138839  0.567138   \n",
       "1945 -0.209269  0.842139  1.558879  0.182652  0.154298  0.389172  0.716143   \n",
       "1946  2.255192  0.358737  0.617064  1.247870 -1.670651 -2.126057 -2.286985   \n",
       "1947  0.023606  1.171070  1.859592 -0.033978 -0.854261 -0.382188 -0.466211   \n",
       "1948 -0.014666  0.404614 -0.374453 -0.756655  0.261750  0.291526 -0.272998   \n",
       "\n",
       "           7         8         9    ...       180       181       182  \\\n",
       "0     0.111030 -0.139985  0.260398  ... -1.232015 -1.458734 -0.386383   \n",
       "1     0.099436  0.549521  0.899578  ... -0.398668 -0.524623 -0.095913   \n",
       "2    -0.588274 -0.491041  0.768550  ... -1.117857 -0.248518 -0.293978   \n",
       "3    -1.162142 -1.058135 -1.205914  ...  0.503463  1.439656  1.222777   \n",
       "4     0.534995  0.603220  0.982447  ... -1.702604 -1.060881 -0.372230   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1944  1.059792  1.223815  0.532453  ... -0.770540 -1.003703 -0.707822   \n",
       "1945  0.767077 -0.211884 -0.146379  ...  0.344871 -0.065061  0.120734   \n",
       "1946 -2.541593 -2.302972 -1.766326  ...  1.469044  2.457780  3.077604   \n",
       "1947  0.073773  1.034530  0.230769  ...  0.837939  0.639342  0.649665   \n",
       "1948 -0.161894 -0.128327 -0.241474  ... -0.616012  0.823558 -1.015916   \n",
       "\n",
       "           183       184       185       186       187       188       189  \n",
       "0    -1.335666 -1.373209 -1.175288 -0.711562 -0.066101 -0.012779  0.290658  \n",
       "1    -0.112957 -0.236553  0.978520  0.038124  0.557227  0.458210  0.248346  \n",
       "2     1.654044  1.383427 -1.181635  0.061036 -2.047093 -2.262965 -1.370896  \n",
       "3     0.164269  0.609022  0.530499  1.688916 -0.602248 -0.772550 -0.377887  \n",
       "4    -0.878557 -0.435961  1.081869  1.522988 -0.810979 -0.455997 -0.217723  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1944 -0.545867 -0.852259 -0.405623  0.283348  0.169265  0.513147  1.067873  \n",
       "1945 -0.526485 -1.161887  0.527328  1.200383  0.463728 -0.203912 -0.892584  \n",
       "1946  4.008954  4.505159  2.485716  4.754478 -1.079503 -1.091256 -0.686173  \n",
       "1947  0.259715  1.365020 -0.204479 -0.166559 -0.079082  0.604597  0.893364  \n",
       "1948 -0.753383 -0.390859 -0.159656 -0.541590  0.858180  0.236513 -0.493873  \n",
       "\n",
       "[1949 rows x 190 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_scaled_df = pd.DataFrame(features_scaled) # make it pretty for display\n",
    "features_scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1rZFd-brPRB"
   },
   "source": [
    "### Dataset Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ux_q_akgryM1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "############# Unscaled test/train set #############\n",
    "#X_train, X_val, y_train, y_val = train_test_split(\n",
    "#    X, \n",
    "#    y, \n",
    "#    test_size=0.2, \n",
    "#    stratify=y\n",
    "#)\n",
    "\n",
    "############ Standard Scaled test/train set ###########\n",
    "# The labels/classes (y_train, y_test) never change, keep old values \n",
    "X_train_scaled, X_test_scaled, y_train, y_val = train_test_split(\n",
    "    features_scaled, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAa3LbhCqTBB"
   },
   "source": [
    "## Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "Jr7sG7UJuDkm",
    "outputId": "1c33fd69-9438-4a72-c9b0-7485d377b4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moad\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>62.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>60.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC RBF kernel</td>\n",
       "      <td>58.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>50.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>48.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>43.59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>35.64%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Classifier Accuracy Score\n",
       "3         RandomForestClassifier         62.31%\n",
       "0           KNeighborsClassifier         60.26%\n",
       "1                 SVC RBF kernel         58.21%\n",
       "4             AdaBoostClassifier         50.00%\n",
       "2         DecisionTreeClassifier         48.72%\n",
       "6  QuadraticDiscriminantAnalysis         43.59%\n",
       "5                     GaussianNB         35.64%"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(\n",
    "    features_scaled, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "classification_models = [\n",
    "    KNeighborsClassifier(),#(3),\n",
    "    #SVC(kernel='linear'),#, C=0.025),\n",
    "    SVC(kernel='rbf'),\n",
    "    DecisionTreeClassifier(),#max_depth=5),\n",
    "    RandomForestClassifier(),#max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "scores = []\n",
    "for model in classification_models:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    score = model.score(X_val_scaled, y_val)\n",
    "    model_name = type(model).__name__\n",
    "    if model_name=='SVC' and model.kernel=='rbf': model_name+=' RBF kernel'\n",
    "    scores.append((model_name,(f'{100*score:.2f}%')))\n",
    "    \n",
    "# Make it pretty\n",
    "scores_df = pd.DataFrame(scores,columns=['Classifier','Accuracy Score'])\n",
    "scores_df.sort_values(by='Accuracy Score',axis=0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuVhMIirqkKZ"
   },
   "source": [
    "### GridSearchCV SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fImyTfKunFO",
    "outputId": "43661f32-0d30-4f77-feaf-cc48c83650d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=1, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=1, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=1, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=2, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=2, gamma=1, kernel=rbf, total=   0.9s\n",
      "[CV] C=2, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=2, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=2, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=2, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=2, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=2, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=2, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=2, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=2, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=2, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=2, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=2, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=2, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=2, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=2, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=2, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=2, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=2, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=2, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=2, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=3, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=3, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=3, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=3, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=3, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=3, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=3, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=3, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=3, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=3, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=3, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=3, gamma=0.1, kernel=rbf, total=   0.7s\n",
      "[CV] C=3, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=3, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=3, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=3, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=3, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=3, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=3, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=3, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=3, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=3, gamma=0.01, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=4, gamma=1, kernel=rbf, total=   0.9s\n",
      "[CV] C=4, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=4, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=4, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=4, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=4, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=4, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=4, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=4, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=4, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=4, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=4, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=4, gamma=0.01, kernel=rbf, total=   0.6s\n",
      "[CV] C=4, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=4, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=4, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=4, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=4, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=4, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=4, gamma=0.01, kernel=rbf .....................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=4, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=5, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=5, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=5, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=5, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=5, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=5, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=5, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=5, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=5, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=5, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=5, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=5, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=5, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=5, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=5, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=5, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=5, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=5, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=5, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=5, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=5, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=6, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=6, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=6, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=6, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=6, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=6, gamma=0.1, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=6, gamma=0.1, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=6, gamma=0.1, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=6, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=6, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=6, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=6, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=6, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=6, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=6, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=6, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=6, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=7, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=7, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=7, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=7, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=7, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=7, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=7, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=7, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=7, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=7, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=7, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=7, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=7, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=7, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=7, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=7, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=7, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=7, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=7, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=7, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=7, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=8, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=8, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=8, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=8, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=8, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=1, kernel=rbf ........................................\n",
      "[CV] ......................... C=8, gamma=1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=8, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=8, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=8, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=8, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=8, gamma=0.1, kernel=rbf, total=   0.8s\n",
      "[CV] C=8, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=8, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=8, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=8, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=8, gamma=0.01, kernel=rbf .....................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=8, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=8, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=8, gamma=0.01, kernel=rbf, total=   0.7s\n",
      "[CV] C=8, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=8, gamma=0.01, kernel=rbf, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "SVC(C=8, gamma=0.01)\n",
      "Test dataset accuracy of best hyperparameter setting: 0.6897435897435897.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "param_grid = { 'C':[1,2,3,4,5,6,7,8],'kernel':['rbf'],'gamma': [ 1, 0.1, 0.01 ]}\n",
    "\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, cv= 5)\n",
    "grid.fit(X_train_scaled,y_train)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    "print('Test dataset accuracy of best hyperparameter setting: {0}.'.format(grid.score(X_val_scaled, y_val)))\n",
    "\n",
    "#grid_predictions = grid.predict(X_val)\n",
    "#print(confusion_matrix(y_val,grid_predictions))\n",
    "#print(classification_report(y_val,grid_predictions))#Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iIpQWCZqtbr"
   },
   "source": [
    "### GridSearchCV RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BEBZ28IxrQ6",
    "outputId": "3ab39022-1459-490c-b018-15fe22ff94de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5791738807815979\n",
      "{'max_features': 'sqrt', 'n_estimators': 200}\n",
      "RandomForestClassifier(max_features='sqrt', n_estimators=200, n_jobs=-1,\n",
      "                       oob_score=True)\n",
      "Test dataset accuracy (random forest classifier): 0.6384615384615384.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200,250,280,300,350],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)\n",
    "print('Test dataset accuracy (random forest classifier): {0}.'.format(grid.score(X_val_scaled, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-IfmUSUUPi-"
   },
   "source": [
    "## Advanced Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementaion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tried to classify numeral unsuccessfully using pytorch RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataloader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data,label ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.datas = torch.tensor(data,dtype=torch.float)\n",
    "        self.labels = torch.tensor(label,dtype=torch.long) \n",
    "        self.length = len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        a = self.datas[index]\n",
    "        b = self.labels[index]\n",
    "\n",
    "        return (a, b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6533, 40, 1)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = X.reshape(6533,40,1 )\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "customData = CustomDataset( X1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = customData, batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = customData, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n"
     ]
    }
   ],
   "source": [
    "for data,label in train_loader:\n",
    "    #print(data)\n",
    "    #print(data.shape)\n",
    "    print('lol')\n",
    "    #print(label)\n",
    "    #print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         print(input_size)\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        print(batch_size)\n",
    "        print(x.size())\n",
    "        #print()\n",
    "\n",
    "#         x = x.view()\n",
    "        # embeddings and lstm_out\n",
    "#         embeds = self.embedding(x)\n",
    "\n",
    "#         x = x.view(4, batch_size, 48)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "#         print(lstm_out.shape)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.logsoftmax(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "#         sig_out = sig_out.view(batch_size, -1)\n",
    "#         sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moad\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "input_size = 40\n",
    "output_size = 1\n",
    "hidden_dim = 1\n",
    "n_layers = 1\n",
    "model = RNN(input_size, output_size, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "lr = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yesd\n",
      "1\n",
      "torch.Size([1, 40, 1])\n",
      "yes2\n",
      "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x0000025D2A296880>>\n",
      "tensor([4])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (40) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-d722284eae6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#         labels = (labels.unsqueeze(0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m#print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2380\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2381\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2382\u001b[0m             \u001b[1;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2383\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (40) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "batch_size = 1\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for data, label in train_loader:\n",
    "             \n",
    "        inputs = data\n",
    "        labels = label \n",
    "        inputs = inputs#.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        print(\"yesd\")\n",
    "        \n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        print(\"yes2\")\n",
    "        \n",
    "        print(input)\n",
    "        print(label)\n",
    "        print(output)\n",
    "#         output = (output.unsqueeze(0))\n",
    "#         labels = (labels.unsqueeze(0))\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "    \n",
    "        #print(loss)\n",
    "    \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        print(\"yes3\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "    \n",
    "            for data, label in train_loader:\n",
    "             \n",
    "                inputs = data\n",
    "                labels = label \n",
    "                \n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                inputs = inputs#.unsqueeze(0)\n",
    "\n",
    "                \n",
    "                if train_on_gpu:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                output, val_h = model(inputs, val_h)\n",
    "                \n",
    "                \n",
    "                #print(output)\n",
    "                val_loss = criterion(output, labels)\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            #print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "            #      \"Step: {}...\".format(counter),\n",
    "            #      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "            #     \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the RNN\n",
    "def train(rnn, n_steps, print_every):\n",
    "    \n",
    "    # initialize the hidden state\n",
    "         \n",
    "    hidden = None\n",
    "    for data, label in train_loader:\n",
    "             \n",
    "        inputs = data\n",
    "        labels = label       \n",
    "        \n",
    "#         # defining the training data \n",
    "#         time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)\n",
    "#         data = np.sin(time_steps)\n",
    "#         data.resize((seq_length + 1, 1)) # input_size=1\n",
    "\n",
    "#         x = data[:-1]\n",
    "#         y = data[1:]\n",
    "        \n",
    "        # convert data into Tensors\n",
    "#         x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
    "#         y_tensor = torch.Tensor(y)\n",
    "\n",
    "        inputs = inputs#.unsqueeze(0)\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        \n",
    "        if True:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "#         print(inputs)\n",
    "#         print(inputs.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "            \n",
    "        \n",
    "        # outputs from the rnn\n",
    "        prediction, hidden = rnn(inputs, hidden)\n",
    "\n",
    "        \n",
    "#         print(\"yes\")\n",
    "        ## Representing Memory ##\n",
    "        # make a new variable for hidden and detach the hidden state from its history\n",
    "        # this way, we don't backpropagate through the entire history\n",
    "        hidden = hidden.data\n",
    "\n",
    "#         print(prediction)\n",
    "#         print(labels.unsqueeze(0))\n",
    "        # calculate the loss\n",
    "        \n",
    "        print('prediction: ',prediction)\n",
    "        print('labels: ', label)\n",
    "        \n",
    "        loss = criterion(prediction, labels)\n",
    "        \n",
    "                \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # perform backprop and update weights\n",
    "        loss.backward()\n",
    "               \n",
    "#         rnn.to(\"cpu\")\n",
    "        \n",
    "        optimizer.step()\n",
    "       \n",
    "               \n",
    "        # display loss and predictions\n",
    "                \n",
    "        print('Loss: ', loss.item())\n",
    "#         plt.plot(time_steps[1:], x, 'r.') # input\n",
    "#         plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') # predictions\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 75\n",
    "print_every = 15\n",
    "\n",
    "trained_rnn = train(rnn, n_steps, print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "hidden = None  \n",
    "print(len(test_loader))\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for X, y in test_loader:\n",
    "       \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "    \n",
    "    output,hidden = trained_rnn(X, hidden)\n",
    "\n",
    "    for idx, i in enumerate(output):\n",
    "        if torch.argmax(i) == y[idx]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
